# -*- coding: utf-8 -*-
"""
Spark MySQLè¿æ¥å™¨
åŸºäºSparkçš„MySQLæ•°æ®è¯»å–å’Œå¤„ç†æ¨¡å—
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import mysql.connector
from mysql.connector import Error
import pandas as pd
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SparkMySQLConnector:
    def __init__(self, mysql_config=None):
        """åˆå§‹åŒ–Spark MySQLè¿æ¥å™¨"""
        self.mysql_config = mysql_config or {
            'host': 'localhost',
            'port': 3306,
            'user': 'root',
            'password': 'wujiayun1',
            'database': 'agricultural_db'
        }
        
        # åˆå§‹åŒ–Sparkä¼šè¯
        self.spark = SparkSession.builder \
            .appName("æ²ƒåœŸè§„åˆ’å¸ˆå†œä¸šåˆ†æç³»ç»Ÿ") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .getOrCreate()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        self.spark.sparkContext.setLogLevel("WARN")
        
        logger.info("âœ… Sparkä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
    
    def read_mysql_table(self, table_name, conditions=None):
        """ä»MySQLè¯»å–è¡¨æ•°æ®åˆ°Spark DataFrame"""
        try:
            # æ„å»ºJDBC URL
            jdbc_url = f"jdbc:mysql://{self.mysql_config['host']}:{self.mysql_config['port']}/{self.mysql_config['database']}"
            
            # JDBCè¿æ¥å±æ€§
            properties = {
                "user": self.mysql_config['user'],
                "password": self.mysql_config['password'],
                "driver": "com.mysql.cj.jdbc.Driver",
                "characterEncoding": "utf8",
                "useUnicode": "true"
            }
            
            # æ„å»ºæŸ¥è¯¢
            if conditions:
                query = f"(SELECT * FROM {table_name} WHERE {conditions}) AS subquery"
            else:
                query = table_name
            
            # è¯»å–æ•°æ®
            df = self.spark.read \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("dbtable", query) \
                .option("user", properties["user"]) \
                .option("password", properties["password"]) \
                .option("driver", properties["driver"]) \
                .load()
            
            logger.info(f"âœ… æˆåŠŸè¯»å–è¡¨ {table_name}: {df.count()} æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–MySQLè¡¨å¤±è´¥: {e}")
            return None
    
    def read_all_agricultural_data(self):
        """è¯»å–æ‰€æœ‰å†œä¸šæ•°æ®"""
        try:
            logger.info("ğŸš€ å¼€å§‹è¯»å–æ‰€æœ‰å†œä¸šæ•°æ®...")
            
            # è¯»å–å„ä¸ªè¡¨
            climate_df = self.read_mysql_table("climate_data")
            soil_df = self.read_mysql_table("soil_data")
            crop_df = self.read_mysql_table("crop_requirements")
            suitability_df = self.read_mysql_table("suitability_results")
            
            # åˆ›å»ºä¸´æ—¶è§†å›¾ç”¨äºSQLæŸ¥è¯¢
            if climate_df:
                climate_df.createOrReplaceTempView("climate_data")
            if soil_df:
                soil_df.createOrReplaceTempView("soil_data")
            if crop_df:
                crop_df.createOrReplaceTempView("crop_requirements")
            if suitability_df:
                suitability_df.createOrReplaceTempView("suitability_results")
            
            logger.info("âœ… æ‰€æœ‰å†œä¸šæ•°æ®è¯»å–å®Œæˆå¹¶åˆ›å»ºä¸´æ—¶è§†å›¾")
            
            return {
                'climate': climate_df,
                'soil': soil_df,
                'crop': crop_df,
                'suitability': suitability_df
            }
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–å†œä¸šæ•°æ®å¤±è´¥: {e}")
            return None
    
    def analyze_climate_trends(self):
        """åˆ†ææ°”å€™è¶‹åŠ¿"""
        try:
            logger.info("ğŸ“Š å¼€å§‹åˆ†ææ°”å€™è¶‹åŠ¿...")
            
            # å¹´åº¦æ¸©åº¦è¶‹åŠ¿
            temp_trend = self.spark.sql("""
                SELECT 
                    year,
                    ROUND(AVG(temperature), 2) as avg_temp,
                    ROUND(MIN(temperature), 2) as min_temp,
                    ROUND(MAX(temperature), 2) as max_temp,
                    COUNT(*) as record_count
                FROM climate_data 
                WHERE year IS NOT NULL AND temperature IS NOT NULL
                GROUP BY year 
                ORDER BY year
            """)
            
            # æœˆåº¦é™æ°´æ¨¡å¼
            precip_pattern = self.spark.sql("""
                SELECT 
                    month,
                    ROUND(AVG(precipitation), 2) as avg_precip,
                    ROUND(STDDEV(precipitation), 2) as precip_std,
                    COUNT(*) as record_count
                FROM climate_data 
                WHERE month IS NOT NULL AND precipitation IS NOT NULL
                GROUP BY month 
                ORDER BY month
            """)
            
            # åœ°åŒºæ°”å€™å·®å¼‚
            regional_climate = self.spark.sql("""
                SELECT 
                    ROUND(lat, 1) as lat_zone,
                    ROUND(lon, 1) as lon_zone,
                    ROUND(AVG(avg_temperature), 2) as avg_temp,
                    ROUND(AVG(annual_precipitation), 2) as avg_precip,
                    COUNT(*) as record_count
                FROM climate_data 
                WHERE lat IS NOT NULL AND lon IS NOT NULL
                GROUP BY ROUND(lat, 1), ROUND(lon, 1)
                HAVING COUNT(*) >= 5
                ORDER BY lat_zone, lon_zone
            """)
            
            logger.info("âœ… æ°”å€™è¶‹åŠ¿åˆ†æå®Œæˆ")
            
            return {
                'temperature_trend': temp_trend,
                'precipitation_pattern': precip_pattern,
                'regional_climate': regional_climate
            }
            
        except Exception as e:
            logger.error(f"âŒ æ°”å€™è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return None
    
    def analyze_soil_distribution(self):
        """åˆ†æåœŸå£¤åˆ†å¸ƒ"""
        try:
            logger.info("ğŸŒ± å¼€å§‹åˆ†æåœŸå£¤åˆ†å¸ƒ...")
            
            # åœŸå£¤ç±»å‹åˆ†å¸ƒ
            soil_type_dist = self.spark.sql("""
                SELECT 
                    soil_type,
                    county,
                    COUNT(*) as count,
                    ROUND(AVG(ph_value), 2) as avg_ph,
                    ROUND(AVG(organic_matter), 2) as avg_organic_matter,
                    ROUND(AVG(total_nitrogen), 3) as avg_nitrogen,
                    ROUND(AVG(available_phosphorus), 2) as avg_phosphorus,
                    ROUND(AVG(available_potassium), 2) as avg_potassium
                FROM soil_data 
                WHERE soil_type IS NOT NULL
                GROUP BY soil_type, county
                ORDER BY soil_type, count DESC
            """)
            
            # pHå€¼åˆ†å¸ƒç»Ÿè®¡
            ph_distribution = self.spark.sql("""
                SELECT 
                    CASE 
                        WHEN ph_value < 5.5 THEN 'å¼ºé…¸æ€§(<5.5)'
                        WHEN ph_value < 6.5 THEN 'é…¸æ€§(5.5-6.5)'
                        WHEN ph_value < 7.5 THEN 'ä¸­æ€§(6.5-7.5)'
                        WHEN ph_value < 8.5 THEN 'ç¢±æ€§(7.5-8.5)'
                        ELSE 'å¼ºç¢±æ€§(>8.5)'
                    END as ph_category,
                    COUNT(*) as count,
                    ROUND(AVG(organic_matter), 2) as avg_organic_matter
                FROM soil_data 
                WHERE ph_value IS NOT NULL
                GROUP BY 
                    CASE 
                        WHEN ph_value < 5.5 THEN 'å¼ºé…¸æ€§(<5.5)'
                        WHEN ph_value < 6.5 THEN 'é…¸æ€§(5.5-6.5)'
                        WHEN ph_value < 7.5 THEN 'ä¸­æ€§(6.5-7.5)'
                        WHEN ph_value < 8.5 THEN 'ç¢±æ€§(7.5-8.5)'
                        ELSE 'å¼ºç¢±æ€§(>8.5)'
                    END
                ORDER BY count DESC
            """)
            
            # å¿å¸‚åœŸå£¤è´¨é‡æ’å
            county_soil_quality = self.spark.sql("""
                SELECT 
                    county,
                    COUNT(*) as sample_count,
                    ROUND(AVG(ph_value), 2) as avg_ph,
                    ROUND(AVG(organic_matter), 2) as avg_organic_matter,
                    ROUND(AVG(total_nitrogen), 3) as avg_nitrogen,
                    ROUND(AVG(available_phosphorus), 2) as avg_phosphorus,
                    ROUND(AVG(available_potassium), 2) as avg_potassium,
                    ROUND(
                        (AVG(organic_matter) * 0.3 + 
                         AVG(total_nitrogen) * 1000 * 0.3 + 
                         AVG(available_phosphorus) * 0.2 + 
                         AVG(available_potassium) * 0.2), 2
                    ) as soil_quality_score
                FROM soil_data 
                WHERE county IS NOT NULL
                GROUP BY county
                HAVING COUNT(*) >= 10
                ORDER BY soil_quality_score DESC
            """)
            
            logger.info("âœ… åœŸå£¤åˆ†å¸ƒåˆ†æå®Œæˆ")
            
            return {
                'soil_type_distribution': soil_type_dist,
                'ph_distribution': ph_distribution,
                'county_soil_quality': county_soil_quality
            }
            
        except Exception as e:
            logger.error(f"âŒ åœŸå£¤åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return None
    
    def analyze_crop_suitability(self):
        """åˆ†æä½œç‰©é€‚å®œæ€§"""
        try:
            logger.info("ğŸŒ¾ å¼€å§‹åˆ†æä½œç‰©é€‚å®œæ€§...")
            
            # ä½œç‰©é€‚å®œæ€§ç»Ÿè®¡
            crop_suitability_stats = self.spark.sql("""
                SELECT 
                    crop_name,
                    suitability_level,
                    COUNT(*) as area_count,
                    ROUND(AVG(comprehensive_suitability), 4) as avg_suitability,
                    ROUND(AVG(temp_suitability), 4) as avg_temp_suitability,
                    ROUND(AVG(precip_suitability), 4) as avg_precip_suitability,
                    ROUND(AVG(soil_suitability), 4) as avg_soil_suitability
                FROM suitability_results 
                GROUP BY crop_name, suitability_level
                ORDER BY crop_name, avg_suitability DESC
            """)
            
            # æœ€ä½³ç§æ¤åŒºåŸŸæ¨è
            best_planting_areas = self.spark.sql("""
                SELECT 
                    crop_name,
                    county,
                    COUNT(*) as suitable_points,
                    ROUND(AVG(comprehensive_suitability), 4) as avg_suitability,
                    ROUND(AVG(lat), 4) as center_lat,
                    ROUND(AVG(lon), 4) as center_lon
                FROM suitability_results 
                WHERE suitability_level IN ('é«˜åº¦é€‚å®œ', 'ä¸­åº¦é€‚å®œ')
                GROUP BY crop_name, county
                HAVING COUNT(*) >= 5
                ORDER BY crop_name, avg_suitability DESC
            """)
            
            # åŒºåˆ’ä¼˜åŒ–å»ºè®®
            zoning_optimization = self.spark.sql("""
                SELECT 
                    zone_id,
                    crop_name,
                    COUNT(*) as grid_count,
                    ROUND(AVG(comprehensive_suitability), 4) as avg_suitability,
                    ROUND(AVG(center_lat), 4) as zone_center_lat,
                    ROUND(AVG(center_lon), 4) as zone_center_lon,
                    COLLECT_SET(county) as counties
                FROM suitability_results 
                WHERE zone_id IS NOT NULL
                GROUP BY zone_id, crop_name
                HAVING COUNT(*) >= 10
                ORDER BY zone_id, avg_suitability DESC
            """)
            
            # é™åˆ¶å› å­åˆ†æ
            limiting_factors = self.spark.sql("""
                SELECT 
                    crop_name,
                    CASE 
                        WHEN temp_suitability = LEAST(temp_suitability, precip_suitability, soil_suitability) 
                        THEN 'æ¸©åº¦é™åˆ¶'
                        WHEN precip_suitability = LEAST(temp_suitability, precip_suitability, soil_suitability) 
                        THEN 'é™æ°´é™åˆ¶'
                        ELSE 'åœŸå£¤é™åˆ¶'
                    END as limiting_factor,
                    COUNT(*) as affected_areas,
                    ROUND(AVG(comprehensive_suitability), 4) as avg_suitability
                FROM suitability_results 
                WHERE suitability_level IN ('å‹‰å¼ºé€‚å®œ', 'ä¸é€‚å®œ')
                GROUP BY crop_name, 
                    CASE 
                        WHEN temp_suitability = LEAST(temp_suitability, precip_suitability, soil_suitability) 
                        THEN 'æ¸©åº¦é™åˆ¶'
                        WHEN precip_suitability = LEAST(temp_suitability, precip_suitability, soil_suitability) 
                        THEN 'é™æ°´é™åˆ¶'
                        ELSE 'åœŸå£¤é™åˆ¶'
                    END
                ORDER BY crop_name, affected_areas DESC
            """)
            
            logger.info("âœ… ä½œç‰©é€‚å®œæ€§åˆ†æå®Œæˆ")
            
            return {
                'crop_suitability_stats': crop_suitability_stats,
                'best_planting_areas': best_planting_areas,
                'zoning_optimization': zoning_optimization,
                'limiting_factors': limiting_factors
            }
            
        except Exception as e:
            logger.error(f"âŒ ä½œç‰©é€‚å®œæ€§åˆ†æå¤±è´¥: {e}")
            return None
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        try:
            logger.info("ğŸ“‹ å¼€å§‹ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
            
            # è·å–æ‰€æœ‰åˆ†æç»“æœ
            climate_analysis = self.analyze_climate_trends()
            soil_analysis = self.analyze_soil_distribution()
            crop_analysis = self.analyze_crop_suitability()
            
            # è½¬æ¢ä¸ºPandas DataFrameç”¨äºå¯¼å‡º
            report_data = {}
            
            if climate_analysis:
                report_data['climate'] = {
                    'temperature_trend': climate_analysis['temperature_trend'].toPandas(),
                    'precipitation_pattern': climate_analysis['precipitation_pattern'].toPandas(),
                    'regional_climate': climate_analysis['regional_climate'].toPandas()
                }
            
            if soil_analysis:
                report_data['soil'] = {
                    'soil_type_distribution': soil_analysis['soil_type_distribution'].toPandas(),
                    'ph_distribution': soil_analysis['ph_distribution'].toPandas(),
                    'county_soil_quality': soil_analysis['county_soil_quality'].toPandas()
                }
            
            if crop_analysis:
                report_data['crop'] = {
                    'crop_suitability_stats': crop_analysis['crop_suitability_stats'].toPandas(),
                    'best_planting_areas': crop_analysis['best_planting_areas'].toPandas(),
                    'zoning_optimization': crop_analysis['zoning_optimization'].toPandas(),
                    'limiting_factors': crop_analysis['limiting_factors'].toPandas()
                }
            
            logger.info("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report_data
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def close(self):
        """å…³é—­Sparkä¼šè¯"""
        if self.spark:
            self.spark.stop()
            logger.info("ğŸ”’ Sparkä¼šè¯å·²å…³é—­")


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    print("ğŸŒ± æ²ƒåœŸè§„åˆ’å¸ˆ - Spark MySQLæ•°æ®åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºè¿æ¥å™¨
    connector = SparkMySQLConnector()
    
    try:
        # è¯»å–æ•°æ®
        data = connector.read_all_agricultural_data()
        if not data:
            print("âŒ æ•°æ®è¯»å–å¤±è´¥")
            return
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = connector.generate_comprehensive_report()
        if report:
            print("ğŸ‰ åˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
            
            # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
            if 'climate' in report:
                print("\nğŸ“Š æ°”å€™è¶‹åŠ¿åˆ†æ:")
                print(report['climate']['temperature_trend'].head())
            
            if 'soil' in report:
                print("\nğŸŒ± åœŸå£¤è´¨é‡æ’å:")
                print(report['soil']['county_soil_quality'].head())
            
            if 'crop' in report:
                print("\nğŸŒ¾ ä½œç‰©é€‚å®œæ€§ç»Ÿè®¡:")
                print(report['crop']['crop_suitability_stats'].head())
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
    
    finally:
        connector.close()


if __name__ == '__main__':
    main()
